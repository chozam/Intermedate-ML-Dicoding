# -*- coding: utf-8 -*-
"""BBC-News-Text-Classidication.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kb794a4wwVyMf_0dSHFsjBRNb1oZmIIO

# **BBC News - Text Classification**
"""

import pandas as pd
df = pd.read_csv('https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/bbc-text.csv')

df

import matplotlib.pyplot as plt
import seaborn as sns
x = df.category.value_counts()
sns.barplot(x=x.index, y=x)
plt.gca().set_title('Class Distributions')
plt.gca().set_ylabel('Samples')
plt.gca().set_xlabel('Sentiment')
plt.show()

df.head()

df.info()

"""## **One Hot Encoding <br>**
karena data kategorik
"""

Label = pd.get_dummies(df["category"])
new_df = pd.concat([df, Label], axis=1)
new_df = new_df.drop(columns=['category'])
new_df

text = new_df['text'].values
label = new_df.iloc[:, 1:].values

print(len(text))
print(len(label))

"""## **Split Dataset (Train dan Test)**"""

from sklearn.model_selection import train_test_split
text_latih, text_test, label_latih, label_test = train_test_split(text, label, test_size=0.2)

"""## **Preprocessing Data <br>**
Menghilangkan stop word, tanda baca, dan lemmatization
"""

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
import re

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))
nltk_tokenizer = nltk.RegexpTokenizer(r"\w+") #menghilangkan tanda baca
lemmatizer = WordNetLemmatizer()

filtered_latih = []
filtered_test = []

def processing(token, list_token):
  for string in token:
      word_tokens = word_tokenize(string)
      filtered_sentence = [lemmatizer.lemmatize(w.lower()) for w in word_tokens if w not in stop_words and w.isalpha()]
      filtered_sentence = ' '.join(filtered_sentence)

      new_sentences = nltk_tokenizer.tokenize(filtered_sentence)
      new_sentences = ' '.join(new_sentences)
      # processed_text =  re.sub(r'[^a-zA-Z\s]', '', new_sentences)
      # processed_text = re.sub(r'http\S+', '', processed_text)
      list_token.append(new_sentences)
  return list_token

filtered_latih = processing(text_latih, filtered_latih)
filtered_test = processing(text_test, filtered_test)

"""## **Tokenizer**"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='<oov>')
tokenizer.fit_on_texts(filtered_latih)
tokenizer.fit_on_texts(filtered_test)

sekuens_latih = tokenizer.texts_to_sequences(filtered_latih)
sekuens_test = tokenizer.texts_to_sequences(filtered_test)

padded_latih = pad_sequences(sekuens_latih, padding='post', maxlen=100)
padded_test = pad_sequences(sekuens_test, padding='post', maxlen=100)

"""## **Setting Model**"""

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16, input_length=100),
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.MaxPooling1D(2),
    tf.keras.layers.Conv1D(32, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(2),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax'),
])

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

from keras.callbacks import Callback, EarlyStopping

class MyCallback(Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.93) and (logs.get('accuracy')>0.90):
      self.model.stop_training = True
callback = MyCallback()
early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)

"""## **Training Model**"""

history = model.fit(padded_latih,
                    label_latih,
                    epochs = 75,
                    validation_data = (padded_test, label_test),
                    verbose = 2,
                    callbacks = [callback, early_stopping])

"""## **Visualisasi Accuracy dan Loss**"""

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('Loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()